{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1kAL7e2P4C5s0Fpva_VQS_521DxOLoAxl","authorship_tag":"ABX9TyMSawDpTIt1x5WkH9SNZhG9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Step 1: Tokenize the texts and convert them into sequences of word indices\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","print(texts)\n","print(sequences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PTxlvThXRvx5","executionInfo":{"status":"ok","timestamp":1689923592221,"user_tz":-330,"elapsed":476,"user":{"displayName":"Bhargav Joshi","userId":"12481558726307722336"}},"outputId":"71197ede-328d-4efe-e669-9d7470d55ad4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['This is a positive review.', 'I really enjoyed this movie.', 'What a waste of time.', 'The acting was terrible.', 'The plot was captivating.', 'The movie was fantastic!', 'It was just okay.']\n","[[3, 6, 4, 7, 8], [9, 10, 11, 3, 5], [12, 4, 13, 14, 15], [2, 16, 1, 17], [2, 18, 1, 19], [2, 5, 1, 20], [21, 1, 22, 23]]\n"]}]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"IatFD-Dh7Tnt"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"T3DDrGQ8QIw6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692199320870,"user_tz":-330,"elapsed":19844,"user":{"displayName":"Bhargav Joshi","userId":"12481558726307722336"}},"outputId":"40ed74bb-55c7-4f57-b842-fca8524554e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1/1 [==============================] - 3s 3s/step - loss: 1.0186 - accuracy: 0.5000 - val_loss: 1.4082 - val_accuracy: 0.0000e+00\n","Epoch 2/10\n","1/1 [==============================] - 0s 67ms/step - loss: 0.8550 - accuracy: 0.7500 - val_loss: 1.6886 - val_accuracy: 0.0000e+00\n","Epoch 3/10\n","1/1 [==============================] - 0s 70ms/step - loss: 0.7234 - accuracy: 1.0000 - val_loss: 1.9789 - val_accuracy: 0.0000e+00\n","Epoch 4/10\n","1/1 [==============================] - 0s 68ms/step - loss: 0.6163 - accuracy: 1.0000 - val_loss: 2.2745 - val_accuracy: 0.0000e+00\n","Epoch 5/10\n","1/1 [==============================] - 0s 66ms/step - loss: 0.5274 - accuracy: 1.0000 - val_loss: 2.5714 - val_accuracy: 0.0000e+00\n","Epoch 6/10\n","1/1 [==============================] - 0s 71ms/step - loss: 0.4523 - accuracy: 1.0000 - val_loss: 2.8658 - val_accuracy: 0.0000e+00\n","Epoch 7/10\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3877 - accuracy: 1.0000 - val_loss: 3.1545 - val_accuracy: 0.0000e+00\n","Epoch 8/10\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3311 - accuracy: 1.0000 - val_loss: 3.4346 - val_accuracy: 0.0000e+00\n","Epoch 9/10\n","1/1 [==============================] - 0s 42ms/step - loss: 0.2812 - accuracy: 1.0000 - val_loss: 3.7039 - val_accuracy: 0.0000e+00\n","Epoch 10/10\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2368 - accuracy: 1.0000 - val_loss: 3.9606 - val_accuracy: 0.0000e+00\n","1/1 [==============================] - 0s 30ms/step - loss: 0.9617 - accuracy: 0.5000\n","Test accuracy: 50.00%\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.utils import to_categorical\n","\n","# Sample text data and corresponding labels (replace this with your own dataset)\n","texts = [\n","    \"This is a positive review.\",\n","    \"I really enjoyed this movie.\",\n","    \"What a waste of time.\",\n","    \"The acting was terrible.\",\n","    \"The plot was captivating.\",\n","    \"The movie was fantastic!\",\n","    \"It was just okay.\"\n","]\n","\n","labels = ['positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'neutral']\n","\n","# Step 1: Tokenize the texts and convert them into sequences of word indices\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","# Step 2: Pad the sequences to make them of equal length (required for neural networks)\n","max_sequence_length = max(len(seq) for seq in sequences)\n","padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n","\n","# Step 3: Convert labels into numerical format\n","label_encoder = LabelEncoder()\n","label_encoder.fit(labels)\n","encoded_labels = label_encoder.transform(labels)\n","num_classes = len(np.unique(encoded_labels))\n","one_hot_labels = to_categorical(encoded_labels, num_classes=num_classes)\n","\n","# Step 4: Load GloVe word embeddings\n","embedding_index = {}\n","with open('/content/drive/MyDrive/Colab Notebooks/glove.6B.100d.txt', encoding='utf-8') as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embedding_index[word] = coefs\n","\n","# Step 5: Create the embedding matrix using GloVe word embeddings\n","embedding_dim = 100\n"," # Change this based on the chosen GloVe embeddings dimension\n","vocabulary_size = len(tokenizer.word_index) + 1\n","embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n","for word, i in tokenizer.word_index.items():\n","    embedding_vector = embedding_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# Step 6: Split the data into training and testing sets\n","x_train, x_test, y_train, y_test = train_test_split(\n","    padded_sequences, one_hot_labels, test_size=0.2, random_state=42\n",")\n","\n","# Step 7: Build the LSTM model with pre-trained word embeddings\n","model = Sequential()\n","model.add(Embedding(vocabulary_size, embedding_dim, weights=[embedding_matrix],\n","                    input_length=max_sequence_length, trainable=False))\n","model.add(LSTM(128))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Step 8: Compile and train the model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.fit(x_train, y_train, epochs=10, batch_size=16, validation_split=0.1)\n","\n","# Step 9: Evaluate the model on the test set\n","loss, accuracy = model.evaluate(x_test, y_test)\n","print(f'Test accuracy: {accuracy * 100:.2f}%')"]},{"cell_type":"code","source":["# Step 2: Pad the sequences to make them of equal length (required for neural networks)\n","max_sequence_length = max(len(seq) for seq in sequences)\n","padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n","print(padded_sequences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dI3gALiITGlu","executionInfo":{"status":"ok","timestamp":1692199329205,"user_tz":-330,"elapsed":4,"user":{"displayName":"Bhargav Joshi","userId":"12481558726307722336"}},"outputId":"1d126829-aa23-4a9f-cfe4-c731de9c767e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 3  6  4  7  8]\n"," [ 9 10 11  3  5]\n"," [12  4 13 14 15]\n"," [ 0  2 16  1 17]\n"," [ 0  2 18  1 19]\n"," [ 0  2  5  1 20]\n"," [ 0 21  1 22 23]]\n"]}]},{"cell_type":"code","source":["# Step 3: Convert labels into numerical format\n","label_encoder = LabelEncoder()\n","label_encoder.fit(labels)\n","encoded_labels = label_encoder.transform(labels)\n","num_classes = len(np.unique(encoded_labels))\n","one_hot_labels = to_categorical(encoded_labels, num_classes=num_classes)\n","print(labels)\n","print(encoded_labels)\n","print(num_classes)\n","print(one_hot_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9nMcFU1PTYmg","executionInfo":{"status":"ok","timestamp":1692199328103,"user_tz":-330,"elapsed":429,"user":{"displayName":"Bhargav Joshi","userId":"12481558726307722336"}},"outputId":"6af27d32-5f2f-4522-9281-758f029bc734"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'neutral']\n","[2 2 0 0 2 2 1]\n","3\n","[[0. 0. 1.]\n"," [0. 0. 1.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 1. 0.]]\n"]}]},{"cell_type":"code","source":["# Step 5: Create the embedding matrix using GloVe word embeddings\n","embedding_dim = 100  # Change this based on the chosen GloVe embeddings dimension\n","vocabulary_size = len(tokenizer.word_index) + 1\n","embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n","for word, i in tokenizer.word_index.items():\n","    embedding_vector = embedding_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","print(vocabulary_size )\n","print(embedding_matrix )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdVVY4yUUo7X","executionInfo":{"status":"ok","timestamp":1692199325667,"user_tz":-330,"elapsed":546,"user":{"displayName":"Bhargav Joshi","userId":"12481558726307722336"}},"outputId":"9f3c8470-9869-45d3-9a2e-bca2d9e87717"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["24\n","[[ 0.          0.          0.         ...  0.          0.\n","   0.        ]\n"," [ 0.13717    -0.54286999  0.19419    ... -0.52056998  0.25433999\n","  -0.23759   ]\n"," [-0.038194   -0.24487001  0.72812003 ... -0.1459      0.82779998\n","   0.27061999]\n"," ...\n"," [-0.30664     0.16821     0.98510998 ... -0.38775     0.36916\n","   0.54521   ]\n"," [ 0.075026    0.39324999  0.90314001 ... -0.18816     0.46636\n","   0.66819   ]\n"," [ 0.066894    0.067771    0.67796999 ... -0.049597   -0.30153999\n","   0.21477   ]]\n"]}]}]}